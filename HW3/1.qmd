---
title: "HW3"
format: 
  html:
    embed-resources: true
---

## Github link

https://github.com/yimingliu0117/STATS506

## Import library

```{r}
library(dplyr)
library(knitr)
library(haven)
library(RSQLite)
library(broom)
library(ggplot2)
```

## Problem 1 - Vision

**a** Firstly, I use function **read.xport()** function to read XPT files. Then I use **inner_join()** function to combine these two files, using the SEQN variable for merging. It is showed that total sample size is now 6,980.

```{r}
# Use package "haven" to read the XPT files
VIX_D <- read_xpt("VIX_D.XPT")
DEMO_D <- read_xpt("DEMO_D.XPT")

# Use function inner_join() to get the combined dataframe
# Show the row-number is 6980
Merged_data <- inner_join(VIX_D, DEMO_D, by = "SEQN")
nrow(Merged_data)
```

**b** In this part, I first divide the interval into ten parts and then use the **aggregate()** function to integrate data.

```{r}
#Break the data into ten different intervals
Merged_data$age_break <- cut(Merged_data$DMDHRAGE,
                               breaks = seq(0, 100, by=10),
                               right = FALSE)

#Aggregate data according to age_break
proportions <- aggregate(VIQ170 ~ age_break, 
                         data = Merged_data, 
                         FUN = mean, 
                         na.rm = TRUE)

colnames(proportions) <- c("age_break","proportion_wearers")

#Create table
kable(proportions, format = "html", caption = "Proportion of Respondents Wearing Glasses/Contact Lenses")
```

**c**

```{r}
#clean the data that VIQ220 is NA
#turn the VIQ220 range into 0 to 1
rm_data <- filter(Merged_data, !is.na(VIQ220))
rm_data <- mutate(rm_data, VIQ220_m1 = VIQ220 - 1)

# Fit regression models
model_1 <- glm(VIQ220_m1 ~ RIDAGEYR, data = rm_data)
model_2 <- glm(VIQ220_m1 ~ RIDAGEYR + RIDRETH1 + RIAGENDR, data = rm_data)
model_3 <- glm(VIQ220_m1 ~ RIDAGEYR + RIDRETH1 + RIAGENDR + INDFMPIR, data = rm_data)

model_list <- list(model_1, model_2, model_3)
summary_list <- lapply(model_list, function(model) {
  tidy_model <- tidy(model)
  tidy_model$OR <- exp(tidy_model$estimate)
  tidy_model$p_value <- round(tidy_model$p.value, 3)
  return(tidy_model)
})
odds_ratios <- bind_rows(summary_list)
kable(odds_ratios, format = "html", digits = 2, caption = "Odds Ratios from Logistic Regression Models")
```

**d** Fistly, we test whether the odds of men and women being wears of glasess/contact lenses for distance vision differs.

```{r}
#clean the data that VIQ220 is NA
#turn the VIQ220 range into 0 to 1
rm_data <- filter(Merged_data, !is.na(VIQ220))
rm_data <- mutate(rm_data, VIQ220_m1 = VIQ220 - 1)

#input the intersaction variable
#use anova() to compare these two regression models
model_interaction <- glm(VIQ220_m1 ~ RIAGENDR * RIDAGEYR +                             RIDRETH1 + INDFMPIR, 
                         data = rm_data)
# Output results
anova(model_interaction, model_3)
```

P-Value is 0.6205 from that output, which means it is different between men and women.

```{r}
#test the propotion difference
prop_test <- prop.test(x =  c(sum(rm_data$VIQ220_m1[rm_data$RIAGENDR == 1],
      na.rm = TRUE),                          sum(rm_data$VIQ220_m1[rm_data$RIAGENDR == 2],
    na.rm = TRUE)),
                       n = c(sum(rm_data$RIAGENDR == 1),
                             sum(rm_data$RIAGENDR == 2)))
# Output results
prop_test
```

p-value is 7.291e-16, which is very small. Thus there is no difference between the proportion of men and women.

## Problem 2 - Sakila

**a** There are 1000 movies released in 2006.

```{r}
#import the data
sakila <- dbConnect(SQLite(), "sakila_master.db")

#make SQL query
gg <- function(query) {
  dbGetQuery(sakila, query)
}

gg("
SELECT release_year, 
       COUNT(*) AS movie_count
  FROM film 
 WHERE release_year = (SELECT MIN(release_year) FROM film)  GROUP BY release_year
")
```

**b** Way 1:use SQL + R

```{r}
#import the data
sakila <- dbConnect(SQLite(), "sakila_master.db")

#make SQL query
gg <- function(query) {
  dbGetQuery(sakila, query)
}

movie_data <- gg("
   SELECT g.name AS genre_name,
          COUNT(f.film_id) AS movie_count
     FROM film AS f
     JOIN film_category AS fc ON f.film_id = fc.film_id
     JOIN category AS g ON fc.category_id = g.category_id
    GROUP BY g.name
   ")

#use r to find min(movie_count)
least_common_genre <- subset(movie_data, movie_count == min(movie_count))
least_common_genre
```

Way 2:use SQL query to get the output. There is 51 movies in Music genre.

```{r}
#import the data
sakila <- dbConnect(SQLite(), "sakila_master.db")

#make SQL query
gg <- function(query) {
  dbGetQuery(sakila, query)
}

gg("
   SELECT g.name AS genre_name,
          COUNT(f.film_id) AS movie_count
     FROM film AS f
     JOIN film_category AS fc ON f.film_id = fc.film_id
     JOIN category AS g ON fc.category_id = g.category_id
    GROUP BY g.name
    ORDER BY movie_count
    LIMIT 1;
   ")
```

**c** Way 1:use SQL + R

```{r}
#import the data
sakila <- dbConnect(SQLite(), "sakila_master.db")

#make SQL query
gg <- function(query) {
  dbGetQuery(sakila, query)
}

#get the country dataframe
country_data <- gg("
   SELECT c.country,
          COUNT(DISTINCT cu.customer_id) AS customer_count
     FROM country AS c
     JOIN city AS ci ON c.country_id = ci.country_id
     JOIN address AS a ON ci.city_id = a.city_id
     JOIN customer AS cu ON a.address_id = cu.address_id
    GROUP BY c.country
   ")

#find countries have exactly 13 customers
cw13_customers <- subset(country_data, customer_count == 13)
cw13_customers
```

Way 2:use SQL query to get the output.

```{r}
#import the data
sakila <- dbConnect(SQLite(), "sakila_master.db")

#make SQL query
gg <- function(query) {
  dbGetQuery(sakila, query)
}

gg("
   SELECT c.country,
          COUNT(DISTINCT cu.customer_id) AS customer_count
     FROM country AS c
     JOIN city AS ci ON c.country_id = ci.country_id
     JOIN address AS a ON ci.city_id = a.city_id
     JOIN customer AS cu ON a.address_id = cu.address_id
    GROUP BY c.country
   HAVING COUNT(DISTINCT cu.customer_id) = 13
   ")
```

## Problem 3 - US Records

**a** The proportion of email addresses are hosted at a domain with TLD “.com” is 73.2%.

```{r}
#import the data into R
data <- read.table("US-500.csv", sep = ",", header = TRUE)

#get domain and tld 
data$domain <- sub(".*@", "", data$email)
data$tld <- sub(".*\\.", "", data$domain) 

#compute the proportion
com_proportion <- sum(data$tld == "com") / nrow(data)
com_proportion
```

**b** The proportion of email addresses have at least one non alphanumeric character is 24.8%.

```{r}
#replace "." and "@" with empty space
#find other type of non alphanumeric characters
non_alph_prop <- sum(grepl("[^a-zA-Z0-9]", 
                           gsub("[.@]", "",data$email))) /                  nrow(data)
non_alph_prop
```

**c** Top 5 Area_code is shown as a dataframe.

```{r}
#extract area_code from phone number 
data$area_code <- substr(data$phone1, 1, 3)

#make a dataframe 
#order **area_code** frequency from highest to lowest
code_counts <- table(data$area_code)
topcodes <- as.data.frame(code_counts[order(-code_counts)])
colnames(topcodes) <- c("Area_code", "Count")

#output top 5 area_code 
head(topcodes, 5)
```

**d**Produce a histogram of the log of the apartment numbers for all addresses.

```{r}
#extract apartment number from address
data$apt_num <- as.numeric(sub(".*#(\\d+)$", 
                               "\\1", 
                               sub(".*\\s(\\d+)$", 
                                   "\\1", 
                                   data$address))
)

# Create a histogram of log of apartment numbers
ggplot(data, aes(x = log(apt_num))) +
       geom_histogram(bins = 30, 
                      fill = "yellow", 
                      alpha = 0.5, 
                      color = "black") +
       labs(title = "Histogram of Log of Apartment Numbers",
            x = "Log(Apartment Number)", 
            y = "Frequency")
```

**e**According to the comparison, this is not in accordance with Benford's law.

```{r}
data$lead_dig <-as.numeric(substr(as.character
                                  (data$apt_num),
                                  1, 1))
# Count leading digits
leading_digit_counts <- table(data$lead_dig)

# Create a bar plot for leading digits
barplot(leading_digit_counts / sum(leading_digit_counts), 
        main = "Leading Digit Distribution of Apartment Numbers", 
        xlab = "Leading Digit", 
        ylab = "Proportion")

# Compare with Benford's Law
benford_proportions <- log10(1 + 1 / (1:9))
barplot(benford_proportions, names.arg = 1:9, col = "yellow", 
        main = "Benford's Law Distribution", 
        xlab = "Leading Digit", 
        ylab = "Expected Proportion", 
        add = TRUE)
```
